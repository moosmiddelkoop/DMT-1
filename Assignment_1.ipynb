{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Techniques\n",
    "## Assignment 1\n",
    "\n",
    "### Group 98: Moos Middelkoop, Willem Huijzer, Max Feucht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A: Exploratory Data Analysis\n",
    "\n",
    "Start with exploring the raw data that is available:\n",
    "- Notice all sorts of properties of the dataset: how many records are there, how many\n",
    "attributes, what kinds of attributes are there, ranges of values, distribution of values,\n",
    "relationships between attributes, missing values, and so on. A table is often a suitable\n",
    "way of showing such properties of a dataset. Notice if something is interesting (to you,\n",
    "or in general), make sure you write it down if you find something worth mentioning. <br><br>\n",
    "- Make various plots of the data. Is there something interesting worth reporting? Re-\n",
    "port the figures, discuss what is in them. What meaning do those bars, lines, dots, etc.\n",
    "convey? Please select essential and interesting plots for discussion, as you have limited\n",
    "space for reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AS14.01</td>\n",
       "      <td>2014-02-26 13:00:00.000</td>\n",
       "      <td>mood</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AS14.01</td>\n",
       "      <td>2014-02-26 15:00:00.000</td>\n",
       "      <td>mood</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AS14.01</td>\n",
       "      <td>2014-02-26 18:00:00.000</td>\n",
       "      <td>mood</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AS14.01</td>\n",
       "      <td>2014-02-26 21:00:00.000</td>\n",
       "      <td>mood</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AS14.01</td>\n",
       "      <td>2014-02-27 09:00:00.000</td>\n",
       "      <td>mood</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AS14.01</td>\n",
       "      <td>2014-02-27 12:00:00.000</td>\n",
       "      <td>mood</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AS14.01</td>\n",
       "      <td>2014-02-27 15:00:00.000</td>\n",
       "      <td>mood</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AS14.01</td>\n",
       "      <td>2014-03-21 09:00:00.000</td>\n",
       "      <td>mood</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AS14.01</td>\n",
       "      <td>2014-03-21 11:00:00.000</td>\n",
       "      <td>mood</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AS14.01</td>\n",
       "      <td>2014-03-21 15:00:00.000</td>\n",
       "      <td>mood</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                     time variable  value\n",
       "1   AS14.01  2014-02-26 13:00:00.000     mood    6.0\n",
       "2   AS14.01  2014-02-26 15:00:00.000     mood    6.0\n",
       "3   AS14.01  2014-02-26 18:00:00.000     mood    6.0\n",
       "4   AS14.01  2014-02-26 21:00:00.000     mood    7.0\n",
       "5   AS14.01  2014-02-27 09:00:00.000     mood    6.0\n",
       "6   AS14.01  2014-02-27 12:00:00.000     mood    6.0\n",
       "7   AS14.01  2014-02-27 15:00:00.000     mood    7.0\n",
       "8   AS14.01  2014-03-21 09:00:00.000     mood    6.0\n",
       "9   AS14.01  2014-03-21 11:00:00.000     mood    6.0\n",
       "10  AS14.01  2014-03-21 15:00:00.000     mood    7.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of lines: 376912\t number of columns: 4\n",
      "\n",
      "Number of unique users: 27\n",
      "\n",
      "Unique IDs: ['AS14.01' 'AS14.02' 'AS14.03' 'AS14.05' 'AS14.06' 'AS14.07' 'AS14.08'\n",
      " 'AS14.09' 'AS14.12' 'AS14.13' 'AS14.14' 'AS14.15' 'AS14.16' 'AS14.17'\n",
      " 'AS14.19' 'AS14.20' 'AS14.23' 'AS14.24' 'AS14.25' 'AS14.26' 'AS14.27'\n",
      " 'AS14.28' 'AS14.29' 'AS14.30' 'AS14.31' 'AS14.32' 'AS14.33']\n",
      "\n",
      "Unique years: ['2014']\n",
      "\n",
      "Unique months: ['02' '03' '04' '05' '06']\n",
      "\n",
      "Number of unique month day combinations: 113\n",
      "\n",
      "Unique variables: ['mood' 'circumplex.arousal' 'circumplex.valence' 'activity' 'screen'\n",
      " 'call' 'sms' 'appCat.builtin' 'appCat.communication'\n",
      " 'appCat.entertainment' 'appCat.finance' 'appCat.game' 'appCat.office'\n",
      " 'appCat.other' 'appCat.social' 'appCat.travel' 'appCat.unknown'\n",
      " 'appCat.utilities' 'appCat.weather']\n",
      "\n",
      "Number of unique variables: 19\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "data = pd.read_csv('data/dataset_mood_smartphone.csv', index_col=0)\n",
    "display(data.head(10))\n",
    "\n",
    "# Print the number of rows and columns\n",
    "print(f\"\\nNumber of lines: {data.shape[0]}\\t number of columns: {data.shape[1]}\")\n",
    "\n",
    "# Print the number of unique IDs\n",
    "print(f\"\\nNumber of unique users: {len(data['id'].unique())}\")\n",
    "\n",
    "# Print the unique IDs\n",
    "print(f\"\\nUnique IDs: {data['id'].unique()}\")\n",
    "\n",
    "# See if there are entries from a year that is not 2014\n",
    "print(f\"\\nUnique years: {data['time'].str[:4].unique()}\")\n",
    "\n",
    "# Print unique months\n",
    "print(f\"\\nUnique months: {data['time'].str[5:7].unique()}\")\n",
    "\n",
    "# Print number of unique month day combinations\n",
    "print(f\"\\nNumber of unique month day combinations: {len(data['time'].str[5:10].unique())}\")\n",
    "\n",
    "# Print the unique variables, and the number of them\n",
    "print(f\"\\nUnique variables: {data['variable'].unique()}\")\n",
    "print(f\"\\nNumber of unique variables: {len(data['variable'].unique())}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will to remove all negative datapoints, except for the valence and arousal variables. As we are dealing with either event counts or activity time measures, negative data points make no sense and can be assumed to be erroneous. This also immediately deletes NA values. It is however possibly interesting to see how many missing or and negative values actually were present before they got deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe without variables circumplex.arousal and circumplex.valence\n",
    "data_without_arousal = data[data['variable'] != 'circumplex.arousal']\n",
    "\n",
    "# remove valence too\n",
    "data_without_circumplex = data_without_arousal[data_without_arousal['variable'] != 'circumplex.valence']\n",
    "\n",
    "# remove rows with negative values\n",
    "data_cleaned = data_without_circumplex[data_without_circumplex['value'] >= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get more data for the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: mood\t & 1.0, 10.0 & 6.993 & 1.033 & 0 & 0\\\\\n",
      "Variable: activity\t & 0.0, 1.0 & 0.116 & 0.187 & 0 & 0\\\\\n",
      "Variable: screen\t & 0.0350000858306885, 9867.00699996948 & 75.335 & 253.822 & 0 & 0\\\\\n",
      "Variable: call\t & 1.0, 1.0 & 1.0 & 0.0 & 0 & 0\\\\\n",
      "Variable: sms\t & 1.0, 1.0 & 1.0 & 0.0 & 0 & 0\\\\\n",
      "Variable: appCat.builtin\t & 0.001, 33960.246 & 19.446 & 312.915 & 0 & 3\\\\\n",
      "Variable: appCat.communication\t & 0.006, 9830.777 & 43.344 & 128.913 & 0 & 0\\\\\n",
      "Variable: appCat.entertainment\t & 0.001, 32148.677 & 37.578 & 262.965 & 0 & 1\\\\\n",
      "Variable: appCat.finance\t & 0.131, 355.513 & 21.755 & 39.218 & 0 & 0\\\\\n",
      "Variable: appCat.game\t & 1.003, 5491.793 & 128.392 & 327.145 & 0 & 0\\\\\n",
      "Variable: appCat.office\t & 0.003, 32708.818 & 22.579 & 449.601 & 0 & 0\\\\\n",
      "Variable: appCat.other\t & 0.014, 3892.038 & 25.811 & 112.781 & 0 & 0\\\\\n",
      "Variable: appCat.social\t & 0.094, 30000.906 & 72.402 & 261.552 & 0 & 0\\\\\n",
      "Variable: appCat.travel\t & 0.08, 10452.615 & 45.731 & 246.109 & 0 & 0\\\\\n",
      "Variable: appCat.unknown\t & 0.111, 2239.937 & 45.553 & 119.4 & 0 & 0\\\\\n",
      "Variable: appCat.utilities\t & 0.246, 1802.649 & 18.538 & 60.959 & 0 & 0\\\\\n",
      "Variable: appCat.weather\t & 1.003, 344.863 & 20.149 & 24.943 & 0 & 0\\\\\n"
     ]
    }
   ],
   "source": [
    "all_vars = data_cleaned['variable'].unique()\n",
    "\n",
    "for var in all_vars:\n",
    "    var_mask = data_cleaned['variable'] == var\n",
    "    var_df = data_cleaned[var_mask]\n",
    "\n",
    "    old_var_mask = data['variable'] == var\n",
    "    old_var_df = data[old_var_mask]\n",
    "\n",
    "    value_range = (var_df['value'].min(), var_df['value'].max())\n",
    "    mean_value = var_df['value'].mean()\n",
    "    std_value = var_df['value'].std()\n",
    "\n",
    "    # I just can't get this to work, it doesn't seem to be able to find the NA values\n",
    "    num_na = old_var_df['value'].isnull().sum()\n",
    "    \n",
    "    num_neg = old_var_df['value'].lt(0).sum()\n",
    "\n",
    "    print(f\"Variable: {var}\\t & {value_range[0]}, {value_range[1]} & {round(mean_value, 3)} & {round(std_value, 3)} & {num_na} & {num_neg}\\\\\\\\\")\n",
    "\n",
    "\n",
    "\n",
    "# get the first and last datetime values\n",
    "first_time = data['time'].min()\n",
    "last_time = data['time'].max()\n",
    "\n",
    "# output the results\n",
    "print(\"First time: \", first_time)\n",
    "print(\"Last time: \", last_time)\n",
    "\n",
    "# filter dataframe to only include \"mood\" variable\n",
    "mood_df = data[data['variable'] == 'mood']\n",
    "\n",
    "# get the first and last datetime values for \"mood\" variable\n",
    "first_mood_time = mood_df['time'].min()\n",
    "last_mood_time = mood_df['time'].max()\n",
    "\n",
    "# output the results\n",
    "print(\"First mood time: \", first_mood_time)\n",
    "print(\"Last mood time: \", last_mood_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned data\n",
    "data_cleaned.to_csv('data/cleaned_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1B: Data Cleaning\n",
    "\n",
    "As the insights from Task 1A will have shown, the dataset you analyze contains quite some\n",
    "noise. Values are sometimes missing, and extreme or incorrect values are seen that are likely\n",
    "outliers you may want to remove from the dataset. We will clean the dataset in two steps:\n",
    "- Apply an approach to remove extreme and incorrect values from your dataset. Describe\n",
    "what your approach is, why you consider that to be a good approach, and describe what\n",
    "the result of applying the approach is. <br><br>\n",
    "- Impute the missing values using two different approaches. Describe the approaches\n",
    "and study the impact of applying them to your data. Argue which one of the two ap-\n",
    "proaches would be most suitable and select that one to form your cleaned dataset. Also\n",
    "base yourself on scientific literature for making your choice.\n",
    "Advanced: The advanced dataset contains a number of time series, select approaches to im-\n",
    "pute missing values that are logical for such time series. Also consider what to do with pro-\n",
    "longed periods of missing data in a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Here\n",
    "# impute missing values: forward fill\n",
    "data_ffill = data.fillna(method='ffill')\n",
    "\n",
    "# create datetimeindex from time column in order to be able to interpolate using the time method\n",
    "datetimeindex = pd.to_datetime(data['time'])\n",
    "\n",
    "# replace time column with datetimeindex\n",
    "data.index = datetimeindex\n",
    "\n",
    "# interpolate using time method\n",
    "data_interpolated = data.interpolate(method='time')\n",
    "\n",
    "# save imputed data\n",
    "data_ffill.to_csv('data/imputed_data_v1.csv')\n",
    "data_interpolated.to_csv('data/imputed_data_v2.csv')\n",
    "\n",
    "#  NOW WRITE SOME ABOUT THE PROS AND CONS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 1C: Feature Engineering \n",
    "\n",
    "While we now have a clean dataset, we can still take one step before we move to classification\n",
    "that can in the end help to improve performance, namely feature engineering. As discussed\n",
    "during the lectures, feature engineering is a creative process and can involve for example the\n",
    "transformation of values (e.g. take the log of values given a certain distribution of values) or combining multiple features (e.g. two features that are more valuable combined than the two\n",
    "separate values). Think of a creative feature engineering approach for your dataset, describe\n",
    "it, and apply it. Report on why you think this is a useful enrichment of your dataset. <br>\n",
    "\n",
    "Advanced: Essentially there are two approaches you can consider to create a predictive model\n",
    "using this dataset (which we will do in the next part of this assignment): (1) use a machine\n",
    "learning approach that can deal with temporal data (e.g. ARIMA, recurrent neural networks)\n",
    "or you can try to aggregate the history somehow to create attributes that can be used in a\n",
    "more common machine learning approach (e.g. SVM, decision tree). For instance, you use\n",
    "the average mood during the last five days as a predictor. Ample literature is present in the\n",
    "area of temporal data mining that describes how such a transformation can be made. For\n",
    "the feature engineering, you are going to focus on such a transformation in this part of the\n",
    "assignment. This is illustrated in Figure 1.\n",
    "In the end, we end up with a dataset with a number of training instances per patient (as\n",
    "you have a number of time points for which you can train), i.e. an instance that concerns\n",
    "the mood at t=1, t=2, etc. Of course it depends on your choice of the history you consider\n",
    "relevant from what time point you can start predicting (if you use a windows of 5 days of\n",
    "history to create attributes you cannot create training instances before the 6th day). To come\n",
    "to this dataset, you need to:\n",
    "1. Define attributes that aggregate the history, draw inspiration from the field of temporal\n",
    "data mining.\n",
    "2. Define the target by averaging the mood over the entire day.\n",
    "3. Create an instance-based dataset as described in Figure 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/dataset_mood_smartphone.csv', index_col=0)\n",
    "\n",
    "data['time'] = pd.to_datetime(data['time']) # Convert time column to datetime\n",
    "data['date'] = data['time'].dt.date  # Create new date, month, and day of week columns\n",
    "data['month'] = data['time'].dt.month\n",
    "data['day_of_week'] = data['time'].dt.dayofweek + 1 # Adding 1 to make Monday = 1, Sunday = 7\n",
    "hour = data['time'].dt.hour\n",
    "\n",
    "conditions = [\n",
    "    (hour >= 0) & (hour < 6),   # Night\n",
    "    (hour >= 6) & (hour < 12),  # Morning\n",
    "    (hour >= 12) & (hour < 18), # Afternoon\n",
    "    (hour >= 18) & (hour <= 23) # Evening\n",
    "]\n",
    "choices = ['night', 'morning', 'afternoon', 'evening']\n",
    "\n",
    "data['time_of_day'] = np.select(conditions, choices)\n",
    "# Convert month and day_of_week to categorical values\n",
    "data['month'] = pd.Categorical(data['month'], categories=range(1,13))\n",
    "data['day_of_week'] = pd.Categorical(data['day_of_week'], categories=range(1,8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOME TRASH IDEAS TO KEEP FOR NOW\n",
    "if True:\n",
    "    to_average = ['mood', 'activity', 'circumplex.arousal', 'circumplex.valence']\n",
    "    to_sum = [var for var in data['variable'].unique() if var not in to_average]\n",
    "\n",
    "    # Define a that take 'variable' as input and return either mean or sum de pending on the variable\n",
    "    def calc_agg(group):\n",
    "        if group.name in ['mood', 'circumplex.arousal', 'circumplex.valence', 'activity']:\n",
    "            return group.mean()\n",
    "        else:\n",
    "                return group.mean() \n",
    "    \n",
    "    def agg_func(x):\n",
    "        if x.name == 'mood':\n",
    "            return x.mean()\n",
    "        else:\n",
    "            return x.sum()\n",
    "\n",
    "    # # group by id, date, month, day_of_week, and variable, and apply the aggregation function to each group\n",
    "    # grouped_data = data.groupby(['id', 'date', 'month', 'day_of_week', 'variable'])['value'].apply(agg_func).reset_index()\n",
    "\n",
    "    # # set the index to the grouping columns\n",
    "    # grouped_data.set_index(['id', 'date', 'month', 'day_of_week', 'variable'], inplace=True)\n",
    "\n",
    "    new_data = data.groupby(['id', 'date', 'month', 'day_of_week', 'variable']).agg({'variable': lambda x: x.mean() if x.item in to_average else x.sum()})\n",
    "    # new_data = data.groupby(['id', 'date', 'month', 'day_of_week', 'variable']).agg({'variable': lambda x: x.mean() if x.values in to_average else x.sum()})\n",
    "\n",
    "    # new_data = pd.pivot_table(data, values='value', index=['id', 'date', 'month', 'day_of_week'], columns='variable', aggfunc=calc_agg)\n",
    "    # new_data.reset_index(inplace=True)\n",
    "\n",
    "    print(new_data.head(20))\n",
    "\n",
    "    new_data.to_csv('new_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable       id        date month day_of_week      mood  activity   \n",
      "0         AS14.01  2014-02-17     2           1       NaN       NaN  \\\n",
      "1         AS14.01  2014-02-18     2           2       NaN       NaN   \n",
      "2         AS14.01  2014-02-19     2           3       NaN       NaN   \n",
      "3         AS14.01  2014-02-20     2           4       NaN       NaN   \n",
      "4         AS14.01  2014-02-21     2           5       NaN       NaN   \n",
      "5         AS14.01  2014-02-22     2           6       NaN       NaN   \n",
      "6         AS14.01  2014-02-25     2           2       NaN       NaN   \n",
      "7         AS14.01  2014-02-26     2           3  6.250000       NaN   \n",
      "8         AS14.01  2014-02-27     2           4  6.333333       NaN   \n",
      "9         AS14.01  2014-02-28     2           5       NaN       NaN   \n",
      "10        AS14.01  2014-03-01     3           6       NaN       NaN   \n",
      "11        AS14.01  2014-03-03     3           1       NaN       NaN   \n",
      "12        AS14.01  2014-03-05     3           3       NaN       NaN   \n",
      "13        AS14.01  2014-03-06     3           4       NaN       NaN   \n",
      "14        AS14.01  2014-03-07     3           5       NaN       NaN   \n",
      "15        AS14.01  2014-03-10     3           1       NaN       NaN   \n",
      "16        AS14.01  2014-03-11     3           2       NaN       NaN   \n",
      "17        AS14.01  2014-03-12     3           3       NaN       NaN   \n",
      "18        AS14.01  2014-03-13     3           4       NaN       NaN   \n",
      "19        AS14.01  2014-03-14     3           5       NaN       NaN   \n",
      "20        AS14.01  2014-03-15     3           6       NaN       NaN   \n",
      "21        AS14.01  2014-03-16     3           7       NaN       NaN   \n",
      "22        AS14.01  2014-03-17     3           1       NaN       NaN   \n",
      "23        AS14.01  2014-03-18     3           2       NaN       NaN   \n",
      "24        AS14.01  2014-03-19     3           3       NaN       NaN   \n",
      "25        AS14.01  2014-03-20     3           4       NaN  0.081548   \n",
      "26        AS14.01  2014-03-21     3           5  6.200000  0.134050   \n",
      "27        AS14.01  2014-03-22     3           6  6.400000  0.236880   \n",
      "28        AS14.01  2014-03-23     3           7  6.800000  0.142741   \n",
      "29        AS14.01  2014-03-24     3           1  6.000000  0.078961   \n",
      "30        AS14.01  2014-03-25     3           2  6.750000  0.098374   \n",
      "31        AS14.01  2014-03-26     3           3  6.600000  0.101308   \n",
      "32        AS14.01  2014-03-27     3           4  7.000000  0.159511   \n",
      "33        AS14.01  2014-03-28     3           5  6.400000  0.095698   \n",
      "34        AS14.01  2014-03-29     3           6  8.000000  0.068203   \n",
      "35        AS14.01  2014-03-30     3           7  7.500000  0.049093   \n",
      "36        AS14.01  2014-03-31     3           1  7.400000  0.070505   \n",
      "37        AS14.01  2014-04-01     4           2  6.000000  0.118443   \n",
      "38        AS14.01  2014-04-02     4           3  6.500000  0.101917   \n",
      "39        AS14.01  2014-04-03     4           4  6.400000  0.049238   \n",
      "\n",
      "variable  circumplex.arousal  circumplex.valence        screen  call  ...   \n",
      "0                        NaN                 NaN           NaN   2.0  ...  \\\n",
      "1                        NaN                 NaN           NaN   1.0  ...   \n",
      "2                        NaN                 NaN           NaN   7.0  ...   \n",
      "3                        NaN                 NaN           NaN   2.0  ...   \n",
      "4                        NaN                 NaN           NaN   NaN  ...   \n",
      "5                        NaN                 NaN           NaN   2.0  ...   \n",
      "6                        NaN                 NaN           NaN   3.0  ...   \n",
      "7                  -0.250000            0.750000           NaN   1.0  ...   \n",
      "8                   0.000000            0.333333           NaN   NaN  ...   \n",
      "9                        NaN                 NaN           NaN   4.0  ...   \n",
      "10                       NaN                 NaN           NaN   NaN  ...   \n",
      "11                       NaN                 NaN           NaN   1.0  ...   \n",
      "12                       NaN                 NaN           NaN   1.0  ...   \n",
      "13                       NaN                 NaN           NaN   1.0  ...   \n",
      "14                       NaN                 NaN           NaN   5.0  ...   \n",
      "15                       NaN                 NaN           NaN   1.0  ...   \n",
      "16                       NaN                 NaN           NaN   3.0  ...   \n",
      "17                       NaN                 NaN           NaN   7.0  ...   \n",
      "18                       NaN                 NaN           NaN   4.0  ...   \n",
      "19                       NaN                 NaN           NaN   3.0  ...   \n",
      "20                       NaN                 NaN           NaN   NaN  ...   \n",
      "21                       NaN                 NaN           NaN   NaN  ...   \n",
      "22                       NaN                 NaN           NaN   NaN  ...   \n",
      "23                       NaN                 NaN           NaN   1.0  ...   \n",
      "24                       NaN                 NaN           NaN   1.0  ...   \n",
      "25                       NaN                 NaN   2275.944000   1.0  ...   \n",
      "26                  0.200000            0.200000  17978.907000   6.0  ...   \n",
      "27                  0.600000            0.500000   6142.161000   3.0  ...   \n",
      "28                  0.200000            0.800000   6773.832001   NaN  ...   \n",
      "29                  0.800000            0.000000  15047.351001  10.0  ...   \n",
      "30                  0.500000            0.500000  21475.354999   NaN  ...   \n",
      "31                 -0.200000            0.600000  16423.801000   NaN  ...   \n",
      "32                  0.200000            0.800000  17442.149999   2.0  ...   \n",
      "33                 -0.600000            0.600000   4923.489000   5.0  ...   \n",
      "34                  0.200000            1.000000   8322.622000   4.0  ...   \n",
      "35                 -0.500000            0.750000   4523.214001   NaN  ...   \n",
      "36                  0.000000            0.600000  11836.834000   6.0  ...   \n",
      "37                  0.200000            0.000000  17173.906002   1.0  ...   \n",
      "38                  0.333333            0.666667  21358.511000   4.0  ...   \n",
      "39                 -1.200000            0.000000  21401.835001   4.0  ...   \n",
      "\n",
      "variable  appCat.game  appCat.office  appCat.other  appCat.social   \n",
      "0                 NaN            NaN           NaN            NaN  \\\n",
      "1                 NaN            NaN           NaN            NaN   \n",
      "2                 NaN            NaN           NaN            NaN   \n",
      "3                 NaN            NaN           NaN            NaN   \n",
      "4                 NaN            NaN           NaN            NaN   \n",
      "5                 NaN            NaN           NaN            NaN   \n",
      "6                 NaN            NaN           NaN            NaN   \n",
      "7                 NaN            NaN           NaN            NaN   \n",
      "8                 NaN            NaN           NaN            NaN   \n",
      "9                 NaN            NaN           NaN            NaN   \n",
      "10                NaN            NaN           NaN            NaN   \n",
      "11                NaN            NaN           NaN            NaN   \n",
      "12                NaN            NaN           NaN            NaN   \n",
      "13                NaN            NaN           NaN            NaN   \n",
      "14                NaN            NaN           NaN            NaN   \n",
      "15                NaN            NaN           NaN            NaN   \n",
      "16                NaN            NaN           NaN            NaN   \n",
      "17                NaN            NaN           NaN            NaN   \n",
      "18                NaN            NaN           NaN            NaN   \n",
      "19                NaN            NaN           NaN            NaN   \n",
      "20                NaN            NaN           NaN            NaN   \n",
      "21                NaN            NaN           NaN            NaN   \n",
      "22                NaN            NaN           NaN            NaN   \n",
      "23                NaN            NaN           NaN            NaN   \n",
      "24                NaN            NaN           NaN            NaN   \n",
      "25                NaN            NaN        11.345        807.731   \n",
      "26                NaN        172.206       239.751       4508.500   \n",
      "27                NaN            NaN        98.143        439.632   \n",
      "28                NaN            NaN        72.823        900.839   \n",
      "29                NaN          3.010        66.558       3223.626   \n",
      "30                NaN            NaN       178.819       1919.471   \n",
      "31                NaN            NaN        97.498       4592.059   \n",
      "32                NaN        182.451        58.532        935.381   \n",
      "33            233.036            NaN       225.951        512.741   \n",
      "34                NaN            NaN       169.594        472.888   \n",
      "35                NaN            NaN        74.003        167.685   \n",
      "36             51.176            NaN       268.554       1350.655   \n",
      "37            234.741         92.388       121.190       5040.067   \n",
      "38                NaN         90.481       541.541       3428.792   \n",
      "39                NaN            NaN       213.902       3154.241   \n",
      "\n",
      "variable  appCat.travel  appCat.unknown  appCat.utilities  appCat.weather   \n",
      "0                   NaN             NaN               NaN             NaN  \\\n",
      "1                   NaN             NaN               NaN             NaN   \n",
      "2                   NaN             NaN               NaN             NaN   \n",
      "3                   NaN             NaN               NaN             NaN   \n",
      "4                   NaN             NaN               NaN             NaN   \n",
      "5                   NaN             NaN               NaN             NaN   \n",
      "6                   NaN             NaN               NaN             NaN   \n",
      "7                   NaN             NaN               NaN             NaN   \n",
      "8                   NaN             NaN               NaN             NaN   \n",
      "9                   NaN             NaN               NaN             NaN   \n",
      "10                  NaN             NaN               NaN             NaN   \n",
      "11                  NaN             NaN               NaN             NaN   \n",
      "12                  NaN             NaN               NaN             NaN   \n",
      "13                  NaN             NaN               NaN             NaN   \n",
      "14                  NaN             NaN               NaN             NaN   \n",
      "15                  NaN             NaN               NaN             NaN   \n",
      "16                  NaN             NaN               NaN             NaN   \n",
      "17                  NaN             NaN               NaN             NaN   \n",
      "18                  NaN             NaN               NaN             NaN   \n",
      "19                  NaN             NaN               NaN             NaN   \n",
      "20                  NaN             NaN               NaN             NaN   \n",
      "21                  NaN             NaN               NaN             NaN   \n",
      "22                  NaN             NaN               NaN             NaN   \n",
      "23                  NaN             NaN               NaN             NaN   \n",
      "24                  NaN             NaN               NaN             NaN   \n",
      "25                  NaN          45.173            21.074             NaN   \n",
      "26              915.445             NaN           598.754             NaN   \n",
      "27               37.305             NaN           117.621             NaN   \n",
      "28                  NaN             NaN            30.086          30.386   \n",
      "29              419.805             NaN           178.732             NaN   \n",
      "30                  NaN         235.223           222.893             NaN   \n",
      "31                  NaN             NaN            33.365             NaN   \n",
      "32               47.314             NaN           179.029             NaN   \n",
      "33             1133.009             NaN           301.717             NaN   \n",
      "34               52.435             NaN           600.637             NaN   \n",
      "35                  NaN          66.477            38.296             NaN   \n",
      "36              183.381          29.290           210.305             NaN   \n",
      "37             1614.621             NaN           517.636             NaN   \n",
      "38                  NaN             NaN           341.371             NaN   \n",
      "39               19.082             NaN           182.772             NaN   \n",
      "\n",
      "variable  NaN_streak  days_since_NaN  \n",
      "0                  1               0  \n",
      "1                  2               0  \n",
      "2                  3               0  \n",
      "3                  4               0  \n",
      "4                  5               0  \n",
      "5                  6               0  \n",
      "6                  7               0  \n",
      "7                  0               1  \n",
      "8                  0               1  \n",
      "9                  1               0  \n",
      "10                 2               0  \n",
      "11                 3               0  \n",
      "12                 4               0  \n",
      "13                 5               0  \n",
      "14                 6               0  \n",
      "15                 7               0  \n",
      "16                 8               0  \n",
      "17                 9               0  \n",
      "18                10               0  \n",
      "19                11               0  \n",
      "20                12               0  \n",
      "21                13               0  \n",
      "22                14               0  \n",
      "23                15               0  \n",
      "24                16               0  \n",
      "25                17               0  \n",
      "26                 0               1  \n",
      "27                 0               1  \n",
      "28                 0               1  \n",
      "29                 0               1  \n",
      "30                 0               1  \n",
      "31                 0               1  \n",
      "32                 0               1  \n",
      "33                 0               1  \n",
      "34                 0               1  \n",
      "35                 0               1  \n",
      "36                 0               1  \n",
      "37                 0               1  \n",
      "38                 0               1  \n",
      "39                 0               1  \n",
      "\n",
      "[40 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "to_average = ['id', 'date', 'month', 'day_of_week', 'mood', 'activity', 'circumplex.arousal', 'circumplex.valence']\n",
    "to_sum = [var for var in data['variable'].unique() if var not in to_average]\n",
    "\n",
    "# I use this as to maintain NaN values in the sum columns\n",
    "def calc_sum(group):\n",
    "    return group.sum() \n",
    "        \n",
    "# Create the new dataset\n",
    "new_data_mean = pd.pivot_table(data, values='value', index=['id', 'date', 'month', 'day_of_week'], columns='variable', aggfunc= 'mean')\n",
    "new_data_sum = pd.pivot_table(data, values='value', index=['id', 'date', 'month', 'day_of_week'], columns='variable', aggfunc=calc_sum)\n",
    "new_data_sum.reset_index(inplace=True)\n",
    "new_data_mean.reset_index(inplace=True)\n",
    "\n",
    "# Merge the two datasets on the common columns\n",
    "merged_data = pd.concat([new_data_mean[to_average], new_data_sum[to_sum]], axis=1)\n",
    "\n",
    "# #Rename columns\n",
    "# new_data.columns.name = None\n",
    "# new_data = new_data.rename(columns={'circumplex.arousal': 'arousal', 'circumplex.valence': 'valence'})\n",
    "\n",
    "# # Reorder columns\n",
    "new_data = new_data_sum[['id', 'date', 'month', 'day_of_week', 'mood', 'call', 'sms', 'screen', 'circumplex.arousal', 'activity',\n",
    "                    'circumplex.valence', 'appCat.builtin', 'appCat.communication',\n",
    "                    'appCat.entertainment', 'appCat.finance', 'appCat.game', 'appCat.office',\n",
    "                    'appCat.other', 'appCat.social', 'appCat.travel', 'appCat.unknown',\n",
    "                    'appCat.utilities', 'appCat.weather']]\n",
    "\n",
    "merged_data['NaN_streak'] = merged_data['mood'].isna().groupby((merged_data['mood'].notnull()).cumsum()).cumsum()\n",
    "merged_data['days_since_NaN'] = merged_data['date'].diff().dt.days.where(merged_data['mood'].notna()).fillna(0).astype(int)\n",
    "\n",
    "print(merged_data.head(40))\n",
    "\n",
    "new_data.to_csv('new_data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2A: Application of Classification Algorithms.\n",
    "\n",
    "Identify the target (i.e. the class you want to predict) for your dataset. In case you use the\n",
    "dataset we collected you are free to choose whatever you like. Split up your data in a train\n",
    "and test set and apply two classification algorithms, at least one of them should have been\n",
    "discussed during the lectures. Optimize the hyperparameters of the approaches. Measure\n",
    "and discuss the performance using a performance metric and argue why that is a suitable\n",
    "metric. Describe all steps in your process clearly and fully to make sure it is reproducible.<br>\n",
    "Advanced: For the advanced assignment you go through the same steps (and shape it into\n",
    "a classification problem for predicting the mood of the next day), however you are required\n",
    "to use two different types of classification algorithms, namely one that uses the dataset you\n",
    "formed in Task 1C (e.g. using a random forest) and an algorithm that is inherently temporal\n",
    "(e.g. ARIMA, recurrent neural networks). Also consider a good evaluation setup given the\n",
    "nature of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2B: Winning Classification Algorithms\n",
    "\n",
    "Machine learning techniques that are used in Data Mining projects develop quickly these\n",
    "days. One nice way to track these developments is to see which algorithms win competitions\n",
    "on websites such as Kaggle. Your task is to describe the approach of the winner of one of those\n",
    "competitions that focus on a classification tasks. The following sites might serve as starting\n",
    "points: <br>\n",
    "- http://www.kaggle.com/ - DM competitions\n",
    "- https://www.kdd.org/kdd-cup - KDD Cup <br>\n",
    "\n",
    "You should be able to find other relevant competitions by searching the Web.\n",
    "The main goal is that you can demonstrate that you understand a technique that beats other\n",
    "techniques under certain conditions (specified by the task and data at hand). Here’s what\n",
    "we’d like you to include in the report for this task: <br>\n",
    "\n",
    "- A description of the competition: what competition, when was it held, what data they\n",
    "were using, what task(s) they were solving, what evaluation measure(s) they used.\n",
    "- Who was the winner, what technique did they use?\n",
    "- What was the main idea of the winning approach? (Typically this would come from a\n",
    "paper written by the winners.)\n",
    "- What makes the winning approach stand out, or how is it different from standard, or\n",
    "non-winning methods? <br>\n",
    "Particular rules and points to consider:\n",
    "• A suggestion: 1 page should be more than enough for this task.\n",
    "• Needless to say, but for the record, please do not copy and paste from papers. Always\n",
    "cite (properly) the source of the paper you are using."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Association Rules\n",
    "\n",
    "We have seen the APRIORI algorithm during the lecture that targets finding associations in\n",
    "datasets, predicting that an item is likely to be bought given other items that are in the shop-\n",
    "ping basket already. As mentioned during the lecture, many innovations have been made to\n",
    "improve the APRIORI and other methods. One category of improvements involves grouping\n",
    "of products into higher level product categories (e.g. a Pizza Margherita and Pizza Quattro\n",
    "Formaggio are both pizza’s). Find an approach that aims to do this and describe it. Discuss\n",
    "the pros and cons of such an approach.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Numerical Prediction\n",
    "Similar to Task 2A, apply a machine learning algorithm to your dataset, but now focus on pre-\n",
    "dicting a numerical target. Describe similar details as you have for the classification problem.\n",
    "Highlight the differences you see between the two types of prediction tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5A: Characteristics of Evaluation Metrics: \n",
    "Consider the following two error measures: mean squared error (MSE) and mean absolute\n",
    "error (MAE).\n",
    "- Write down their corresponding formulae.\n",
    "- Discuss: Why would someone use one and not the other?\n",
    "- Describe an example situation (dataset, problem, algorithm perhaps) where using MSE\n",
    "or MAE would give identical results. Justify your answer (some maths may come handy,\n",
    "but clear explanation is also sufficient)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5B: Impact of Evaluation Metrics\n",
    "\n",
    "Apply the MSE and MAE as evaluation metrics to the numerical prediction problem you have\n",
    "worked on under Task 4. Describe how the model behaves under the different characteristics\n",
    "and describe the implications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfd9938a71abe54669f46ed12b810685d517b196090f83ed7ccc56843fca6a13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
